{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LUMI\n",
    "LUMI (or, â€˜Large Unified Modern Infrastructureâ€™) is a large, high performance computing (â€˜HPCâ€™) facility situated in Kajaani, Finland. It opened in 2021, and is one of the most powerful computing facilities in the world (in 2022 it was ranked as the 3rd fastest supercomputer in the world). \n",
    "Denmark is one of the members of the LUMI consortium, meaning that DeIC (Danish e-Infrastruture Consortium), which is the organization that handles national computing infrastructure in Denmark, has access to a certain amount of computing resources on LUMI. They have been to so kind as to share some of those resources with the students of this course. \n",
    "\n",
    "Hence this guide ðŸ˜‰\n",
    "\n",
    "If you are curious about LUMI in general, I recommend browsing this website: https://www.lumi-supercomputer.eu/\n",
    "\n",
    "## First things first:\n",
    "\n",
    "The very first thing to do, if you're interested in using LUMI, is to put your email on the list of interested students. Then I send that list to DeIC, who create toy projects for everyone interested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting ready \n",
    "After the toy projects have been created, it's time to sign up. You should have received an email from 'lumi-noreply@lumi.deic.dk' titled 'Invitation to XXXXXXX project'. Inside there is a link for signing up and accepting your invitation. Please click that and follow the instructions. \n",
    "\n",
    "__Please be aware that there is a time limit for how long the invitation is active__ - usually it's several days, but don't wait a week or more. The link will become inactive, and it's a hassle to have to go through the process again.\n",
    "\n",
    "After you have done this, you will receive a username to LUMI (this may take up to an hour). That will be in an email from 'lumi-noreply@csc.fi' with the words 'Your CSC username is '. Whatever follows immediately after that is your username. Please keep that handy.\n",
    "\n",
    "As hinted at above, using LUMI is not free, and to do anything, we need to tell LUMI which project will be paying for our activities. This means going into an another email that you will receive from 'info-noreply@csc.fi', which has a project ID of the form 'project_465XXXXXX'. We need this project ID whenever we want to do anything substantial with LUMI. For now, simply find the email, and keep it handy for later. \n",
    "\n",
    "(when I signed up, the project ID arrived before the username, I don't know if that is how it always goes)\n",
    "\n",
    "## Accesssing LUMI\n",
    "\n",
    "After receiving the above emails, we have to actually access the machine. There are, fundamentally, two approaches:\n",
    "\n",
    "### Use the web interface\n",
    "\n",
    "Go to https://www.lumi.csc.fi/. \n",
    "Click 'Go to login'. \n",
    "Click on 'MyAccessID' -> go through that\n",
    "Open VS code (gives you both an editor and a terminal)\n",
    "\n",
    "\n",
    "### Use a public/private keypair and an SSH client\n",
    "\n",
    "This is the 'advanced' method which will probably be preferable if you are to work with LUMI for a longer time. First set up an SSH key pair (explained here: https://docs.lumi-supercomputer.eu/firststeps/SSH-keys/). Then, open your favourite SSH client and connect to the lumi.csc.fi server. If you have no favourite, I suggest either installing the SSH plugin for Visual Studio Code, or using 'mobaxterm'. If you get in, you will be greeted by this cool picture of a wolf:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    " *  â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’  *   *      *\n",
    "                                                       *      *  *\n",
    "   * â–ˆâ–ˆâ–ˆâ–ˆ       â–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„    â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆ     *     *\n",
    " *   â–ˆâ–ˆâ–ˆâ–ˆ       â–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆ â–ˆâ–„  â–„â–ˆ â–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆ         ,    *,\n",
    "     â–ˆâ–ˆâ–ˆâ–ˆ       â–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆ  *   *  |\\_ _/|\n",
    "     â–ˆâ–ˆâ–ˆâ–ˆ       â–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆ   â–€â–€   â–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆ   *    .| .\" ,|\n",
    "  *  â–ˆâ–ˆâ–ˆâ–ˆ       â–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆ        â–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆ        /(  \\_\\)\n",
    "     â–ˆâ–ˆâ–ˆâ–ˆ       â–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆ        â–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆ       /    ,-,|\n",
    " *   â–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„  â–€â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–€   â–ˆâ–ˆâ–ˆâ–ˆ        â–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆ *    * /      \\\n",
    "     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    â–€â–€â–ˆâ–ˆâ–ˆâ–€â–€     â–ˆâ–ˆâ–ˆâ–ˆ        â–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆ  * ,/  (      *\n",
    " *                                                     ,/       |  /\n",
    "  * â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’â–’/    \\  * || |\n",
    "                 *              *               ,_   (       )| || |\n",
    "*   *    *    The Supercomputer of the North  * | `\\_|   __ /_| || |\n",
    "        **               *            * *       \\_____\\______)\\__)__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "On LUMI, we work with 'containers'. If you're not familiar with containers, they are a handy way to run code without installing a lot of other libraries and dependencies (which is something the people behind LUMI do not want us to do). \n",
    "\n",
    "If you are curious, the specific type of container which we will be using is the 'Singularity' container: https://github.com/sylabs/singularity (if you're not curious, just skip that link)\n",
    "\n",
    "For convenience, I have already created a container which we will be using in this guide. It contains a python environment with pytorch, numpy, and SKlearn, and you are free to use it for the project too. If you have more elaborate needs for your python environment, look at the bottom of this guide for instructions on how to make your own container. You can download it directly to your LUMI folder using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "wget https://anon.erda.au.dk/share_redirect/FYaG5IWTPR -O cotainrImage.sif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(takes about 50 seconds)\n",
    "\n",
    "In case you were wondering: we build singularity containers using a tool called 'cotainr'. This is the reason for the name of the .sif-file above. \n",
    "\n",
    "# Hello world\n",
    "It's now, finally, time to run a python script on LUMI. We start with the classic 'hello world'. Put the following in a .py file called hello.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#describe the environment that has been loaded:\n",
    "print(\"Hello World!\")\n",
    "\n",
    "print('The name of my current conda environment is:')\n",
    "print(os.environ['CONDA_DEFAULT_ENV'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And run this command in a terminal on LUMI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "singularity exec cotainrImage.sif python hello.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should give you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Hello World!\n",
    "The name of my current conda environment is:\n",
    "conda_container_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far so good. There are two important take aways from this:\n",
    "\n",
    "1: We can run regular python scripts on LUMI without bothering with actually installing python or anaconda or anything else\n",
    "\n",
    "2: This is __the wrong way to do this__.\n",
    "\n",
    "The reason for #2 is that right now, we are actually running code on the 'front-end', which is a big no-no on a cluster such as LUMI. To explain what is going on, I need to show you this overall diagram of how clusters are *supposed* to be used:\n",
    "\n",
    "![cluster diagram](clusterDiagram.png)\n",
    "\n",
    "\n",
    "Here, we log onto the front-end computer (blue box on top), where we send a jobscript, which is a detailed description of the calculation we would like to perform, to a jobmanager (which on LUMI is called 'Slurm'), which then makes sure the calculation is executed on one or more of its many *nodes*. In our case, LUMI has 2928 GPU-enabled nodes, where each node is a quite powerful computer with 512 GB RAM, a processor with 64 cores and 8 GPUs with each 64 GB of dedicated memory. In your projects in this course, you will be using at most one full node. As you can see, the storage is separate from both front-end and nodes, and so our jobscript usually also needs to specify where the data that we need for the computation is located (we'll do that later in this guide). \n",
    "\n",
    "What we did above with our 'hello world' script was to run our job directly on the front end, without bothering the jobmanager. This is fine if you're just downloading data or checking that your code works (like calling hello.py), but you shouldn't do it for anything heavy. \n",
    "\n",
    "Let's run a more elaborate, but also more realistic, example:\n",
    "\n",
    "## Hello mnist\n",
    "\n",
    "First, download mnist data and move it to your scratch folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "wget https://anon.erda.au.dk/share_redirect/AIYv1rmrtI -O mnist.h5\n",
    "mv mnist.h5 /scratch/project_465XXXXXX/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that I have here saved the mnist data (in the form of numpy-arrays) to and hdf5-file, which we then store in the scratch folder. \n",
    "\n",
    "This serves two purposes:\n",
    " - minimizes the bookkeeping necessary for data handling on the cluster (that is nice for us as users)\n",
    " - minimizes the number of individual files stored on LUMI. This is an advantage because many small files (such as a large image dataset, for instance) risks burdening the LUMI file system with overhead (the file system used on LUMI prefers fewer larger files instead of many small).\n",
    "\n",
    "Please use a similar convention in your own projects (and remember to place the data on scratch, you have more space there).\n",
    "\n",
    "Next put this code in a python script called hello_mnist.py:\n",
    "\n",
    "(you're welcome to read it at make sure you understand it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#%% import packages:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "\n",
    "#%% check if cuda is available:\n",
    "if torch.cuda.is_available():\n",
    "    print('cuda is available')\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    print('cuda is not available')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "#%% set up data sets:\n",
    "data=h5py.File('/data/mnist.h5')\n",
    "Xtrain=np.array(data['Xtrain'])\n",
    "Xtest=np.array(data['Xtest'])\n",
    "ytrain=np.array(data['ytrain'])\n",
    "ytest=np.array(data['ytest'])\n",
    "\n",
    "#convert numpy arrays to torch tensors:\n",
    "Xtrain=torch.from_numpy(Xtrain).float()\n",
    "Xtest=torch.from_numpy(Xtest).float()\n",
    "ytrain=torch.from_numpy(ytrain)\n",
    "ytest=torch.from_numpy(ytest)\n",
    "\n",
    "#set up data sets:\n",
    "train_set = torch.utils.data.TensorDataset(Xtrain, ytrain)\n",
    "test_set = torch.utils.data.TensorDataset(Xtest, ytest)\n",
    "\n",
    "#set up data loaders:\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=False)\n",
    "\n",
    "#%% set up sequential model:\n",
    "\n",
    "net=nn.Sequential(\n",
    "        nn.Conv2d(1, 10, kernel_size=5),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(10, 20, kernel_size=5),\n",
    "        nn.Dropout2d(),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.ReLU(),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(320, 50),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(50, 10)\n",
    "                )\n",
    "\n",
    "#%% set up optimizer:\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "\n",
    "#%% set up loss function:\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "#%% train the model:\n",
    "\n",
    "net.to(device)\n",
    "for epoch in range(100):\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        X, labels = data\n",
    "\n",
    "\n",
    "        X=X.to(device)\n",
    "        labels=labels.long().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(X.view(-1, 1, 28, 28))\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(epoch+1, loss.item())\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "#%% test the model:\n",
    "net.to('cpu')\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        outputs = net(inputs.view(-1, 1, 28, 28))\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted==labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (100*correct/total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, put the following in another file, jobscript1.sh:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#SBATCH --job-name=helloMnist\n",
    "#SBATCH --account=project_465XXXXXX\n",
    "#SBATCH --time=00:10:00\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=8\n",
    "#SBATCH --mem=5G\n",
    "#SBATCH --partition=small-g\n",
    "#SBATCH --gpus-per-task=1\n",
    "\n",
    "srun singularity exec -B /scratch/project_465XXXXXX/:/data cotainrImage.sif python hello_mnist.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, we give our job a name ('helloMnist'), define a project to bill (notice that I put in a placeholder, you need to put your own project number), tell slurm how long the job is allowed to run at most (0 hrs, 10 minutes and 0 seconds), tell it to only use 1 node, that our job consists of 1 task, that it should dedicate 8 cpus to this task, 5 GB RAM, 1 GPU and to put it in the 'small-g' queue. \n",
    "\n",
    "For ease of use, I create a variable called 'SCRATCH_FOLDER' indicating where our data is going to be found.\n",
    "\n",
    "In the bottom line, we tell slurm to run the singularity command from before, using the command 'srun'. Note that we tell singularity to 'mount' the scratch folder to the 'data' folder inside the container. This means that anything in the scratch folder will appear (from inside the container), as if it was inside a folder called 'data', in our working directory.\n",
    "\n",
    "We can use this file to make slurm run our python job on a node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "sbatch jobscript1.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates an outputfile, which (because we haven't changed the name) will get a pretty unimaginative name on the form 'slurm-1234567.out'. If we print the contents, we see that it's just the standard-out from our script:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "MIOpen(HIP): Warning [SQLiteBase] Missing system database file: gfx90a6e.kdb Performance may degrade. Please follow instructions to install: https://github.com/ROCmSoftwarePlatform/MIOpen#installing-miopen-kernels-package\n",
    "cuda is available\n",
    "1 1.3244482278823853\n",
    "11 0.7749866843223572\n",
    "21 0.4031255841255188\n",
    "31 0.6772407293319702\n",
    "41 0.5549220442771912\n",
    "51 0.7431323528289795\n",
    "61 0.7124130129814148\n",
    "71 0.6285948157310486\n",
    "81 0.5698408484458923\n",
    "91 0.9437584280967712\n",
    "Finished Training\n",
    "Accuracy of the network on the 10000 test images: 74 %\n",
    "mikkelse@uan02:~/LUMI_guide>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ignore the warning in the beginning)\n",
    "\n",
    "We are really getting somewhere now !\n",
    "\n",
    "In your own jobscripts, you will probably want to increase the max duration (the 'walltime') from 10 minutes to something sensible, like a couple of hours. However, be careful with changing 'mem' and 'gpus'. As you can read at https://docs.lumi-supercomputer.eu/runjobs/lumi_env/billing/, your jobs will be billed based on how many resources you reserve, and not how much you use. And if, for instance, you reserve 512 GB of RAM, which is an entire node, then you will also be billed for using all 8 GPUs on that node. No matter if you only used 7 GB of RAM and a single GPU. This makes it possible to blow through the whole compute allocation for your project really quickly, if you accidentally reserve much more resources than you need. \n",
    "The same goes for increasing the number of nodes, or starting multiple jobs. \n",
    "\n",
    "Note:\n",
    "If you decide to use LUMI for your project, your group will be awarded 200 GPU hours and 20 TB hours (meaning that you can have 20 TB stored for one hour, or about 40 GB for three weeks). It's up to you to make those resources last the full duration of your project. \n",
    "\n",
    "If you, during the project, are curious how much of your resources you have spent already, the command 'lumi-allocations' will give you a handy overview.\n",
    "Another thing to note is that the 8 GPUs on each node are actually counted as 8 half-GPUs. This means that allocating a single GPU for 1 hour actually only costs half an hour... which is a little weird. More details about billing can be found here: https://docs.lumi-supercomputer.eu/runjobs/lumi_env/billing/\n",
    "\n",
    "## student_sbatch\n",
    "Sometimes, particularly when learning to use slurm, you may accidentally create a job which allocates much more compute that you intended to (for instance, accidentally allocating all the memory on a node, even if you're just using a single GPU). This is annoying because you pay for what you reserve, not what you use. To help catch such errors, we have created *student_sbatch*, which is a wrapper for sbatch. It works almost exactly the same as sbatch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "./student_sbatch.sh jobscript1.sh\n",
    "GPU hours billed by this job: .08333333333333333333\n",
    "Total GPU hours allocated: 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, 'student_sbatch' reads jobscript1, calculates what the total number of GPU hours billed would be if the job ran to the very end, and compares that with the allocation of the whole project. If the job requires more than 10% of the full amount (in this case that would be 50 GPU hours), the job is not allowed to run. Otherwise, the jobscript is passed to sbatch which runs it like normal. \n",
    "\n",
    "You obtain student_sbatch by downloading it and making it executable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "wget https://anon.erda.au.dk/share_redirect/CyBLhyxsNa -O student_sbatch.sh\n",
    "chmod +x student_sbatch.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slurmlearning\n",
    "So far, this has been a pretty patchy crash course in the slurm jobmanager. Fortunately, DeIC has made a really excellent slurm tutorial which I recommend anyone who want to use LUMI go through: http://slurmlearning.deic.dk/\n",
    "\n",
    "### You have now finished the compulsory part of the tutorial :)\n",
    "\n",
    "# Other useful details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job arrays\n",
    "A nice thing about working on a cluster is that you can do a lot of things simultaneously. For instance, run a bunch of similar calculations with slightly different hyper parameter values (like when we're doing hyper parameter optimization). \n",
    "\n",
    "One way to organize that is by using 'job arrays', which essentially just tells slurm to start a lot of small jobs independently of each other. We do this by adding one more line to the jobscript:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#SBATCH --job-name=helloMnist\n",
    "#SBATCH --account=project_465000376\n",
    "#SBATCH --time=00:10:00\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=10\n",
    "#SBATCH --mem=5G\n",
    "#SBATCH --partition=small-g\n",
    "#SBATCH --gpus=1\n",
    "#SBATCH --array=1-5\n",
    "\n",
    "\n",
    "srun singularity exec -B /scratch/project_465000376/data cotainrImage.sif \\\n",
    "    python array_mnist.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "saying that this will be an array of 5 (indexed by 1-5) independent jobs. This then creates an environment variable called 'SLURM_ARRAY_TASK_ID' which distinguishes between the 5 jobs. There are many ways of using this, but I think the simplest way is to just access it directly from within our python script:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "os.getenv('SLURM_ARRAY_TASK_ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(note that this is a string)\n",
    "\n",
    "In the example below, I have changed the 'hello_mnist' script to a crude hyper parameter line scan, trying different sizes of the last hidden layer of the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#%% import packages:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "#%% check if cuda is available:\n",
    "if torch.cuda.is_available():\n",
    "    print('cuda is available')\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    print('cuda is not available')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "#%% set up data sets:\n",
    "\n",
    "#load numpy arrays from ./data:\n",
    "Xtrain=np.load('./data/Xtrain.npy')\n",
    "Xtest=np.load('./data/Xtest.npy')\n",
    "ytrain=np.load('./data/ytrain.npy')\n",
    "ytest=np.load('./data/ytest.npy')\n",
    "\n",
    "#convert numpy arrays to torch tensors:\n",
    "Xtrain=torch.from_numpy(Xtrain).float()\n",
    "Xtest=torch.from_numpy(Xtest).float()\n",
    "ytrain=torch.from_numpy(ytrain)\n",
    "ytest=torch.from_numpy(ytest)\n",
    "\n",
    "#set up data sets:\n",
    "train_set = torch.utils.data.TensorDataset(Xtrain, ytrain)\n",
    "test_set = torch.utils.data.TensorDataset(Xtest, ytest)\n",
    "\n",
    "#set up data loaders:\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=False)\n",
    "\n",
    "#%% set up sequential model:\n",
    "\n",
    "sizeArray=[10,20,30,40,50]\n",
    "lastSize=sizeArray[int(os.getenv('SLURM_ARRAY_TASK_ID'))-1]\n",
    "\n",
    "print('lastSize: ',lastSize)\n",
    "\n",
    "net=nn.Sequential(\n",
    "        nn.Conv2d(1, 10, kernel_size=5),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(10, 20, kernel_size=5),\n",
    "        nn.Dropout2d(),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.ReLU(),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(320, lastSize),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(lastSize, 10)\n",
    "                )\n",
    "\n",
    "#%% set up optimizer:\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "\n",
    "#%% set up loss function:\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "#%% train the model:\n",
    "\n",
    "net.to(device)\n",
    "for epoch in range(100):\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        X, labels = data\n",
    "\n",
    "\n",
    "        X=X.to(device)\n",
    "        labels=labels.long().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(X.view(-1, 1, 28, 28))\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "#%% test the model:\n",
    "net.to('cpu')\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        outputs = net(inputs.view(-1, 1, 28, 28))\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted==labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (100*correct/total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a pretty crude way of doing things, but I hope you get the idea. Some slightly more elaborate examples are given here:\n",
    "\n",
    "https://docs.lumi-supercomputer.eu/runjobs/scheduled-jobs/throughput/\n",
    "\n",
    "\n",
    "### How to make a new container\n",
    "If you need other packages installed instead of what is in the container I have supplied, this is the workflow:\n",
    "\n",
    "First you have to describe the conda environment which you intend to build. This has to include both the 'normal' packages that you are interested in, and GPU libraries that work with the AMD GPUs on LUMI (so not CUDA). Below is the 'yaml'-file that I used to create my container:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "name: py311_062024\n",
    "channels:\n",
    "- conda-forge\n",
    "dependencies:\n",
    "- python=3.11\n",
    "- mne\n",
    "- mne-bids\n",
    "- neptune\n",
    "- lightning\n",
    "- matplotlib\n",
    "- numpy\n",
    "- scikit-learn\n",
    "- ray-tune\n",
    "- optuna\n",
    "- tabulate\n",
    "- pandas\n",
    "- pip\n",
    "- h5py\n",
    "- wandb\n",
    "- pyarrow\n",
    "- pip:\n",
    "    - --extra-index-url https://download.pytorch.org/whl/rocm6.0\n",
    "    - torch==2.4+rocm6.0\n",
    "    - torchaudio==2.4+rocm6.0\n",
    "    - torchvision==0.19+rocm6.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was saved in a file called 'py311_092024.yml'. You should probably not touch anything below 'pip'. \n",
    "\n",
    "If you make your own yml file, you then need to load first the crayEnv module and then the cotainr module on the LUMI frontend, after which you can build the container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "module load CrayEnv\n",
    "module load cotainr\n",
    "cotainr build py311_092024.sif --system=lumi-g --conda-env=py311_092024.yml --accept-licenses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a container called py311_092024.sif. Please note that these containers take up a lot of space, and take a while to create (~30 minutes). Your user folder is limited to 20 GB of storage, meaning that two or three containers is the most you can have in your personal storage at a time. If you start creating containers without enough space, the building process will not succeed. So, it's a good idea to just have one container which works for everything. \n",
    "\n",
    "If you need a newer version of of python or pytorch, you are welcome to try changing version requirements in the yml file above. Though, it is possible that may get some really long error messags from cotainr if you pick unlucky combinationws.\n",
    "\n",
    "### Using 'Ray' on LUMI\n",
    "Possibly the most straight-forward use of a machine like LUMI is for hyper parameter optimization, which you will probably want to do towards the end of the project (most projects, really). A really good tool for that is the 'ray' library, which has a subcomponent called 'tune'. In the example below, we return to the optimization problem from before, but we use ray.tune to adjust the size of the last layer and the learning rate:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import ray\n",
    "from ray import tune, air\n",
    "from ray.air import session\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#%%\n",
    "\n",
    "#%% import packages:\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import h5py\n",
    "\n",
    "#%% check resources\n",
    "#\n",
    "# if cuda is available:\n",
    "if torch.cuda.is_available():\n",
    "    print('cuda is available')\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    numGPUs=torch.cuda.device_count()\n",
    "else:\n",
    "    print('cuda is not available')\n",
    "    device = torch.device(\"cpu\")\n",
    "    numGPUs=0\n",
    "\n",
    "#number of cpus:\n",
    "print('SLURM_GPUS_PER_TASK: ',os.getenv('SLURM_GPUS_PER_TASK'))\n",
    "numCPUs=int(os.getenv('SLURM_CPUS_PER_TASK'))\n",
    "print('numCPUs: ',numCPUs)\n",
    "\n",
    "print(os.listdir('/data'))\n",
    "\n",
    "\n",
    "#%% set up data sets:\n",
    "data=h5py.File('/data/mnist.h5')\n",
    "Xtrain=np.array(data['Xtrain'])\n",
    "Xtest=np.array(data['Xtest'])\n",
    "ytrain=np.array(data['ytrain'])\n",
    "ytest=np.array(data['ytest'])\n",
    "\n",
    "#convert numpy arrays to torch tensors:\n",
    "Xtrain=torch.from_numpy(Xtrain).float()\n",
    "Xtest=torch.from_numpy(Xtest).float()\n",
    "ytrain=torch.from_numpy(ytrain)\n",
    "ytest=torch.from_numpy(ytest)\n",
    "\n",
    "#set up data sets:\n",
    "train_set = torch.utils.data.TensorDataset(Xtrain, ytrain)\n",
    "test_set = torch.utils.data.TensorDataset(Xtest, ytest)\n",
    "\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def trainTestNet(config,train_set,test_set):\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    #set up data loaders:\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=False)\n",
    "\n",
    "    net=nn.Sequential(\n",
    "            nn.Conv2d(1, 10, kernel_size=5),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(10, 20, kernel_size=5),\n",
    "            nn.Dropout2d(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(320, config['lastSize']),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config['lastSize'], 10)\n",
    "                    )\n",
    "\n",
    "    optimizer = optim.SGD(net.parameters(), lr=config['lr'], momentum=0.9)\n",
    "\n",
    "    # train the model:\n",
    "    net.to(device)\n",
    "    for epoch in range(100):\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            X, labels = data\n",
    "\n",
    "\n",
    "            X=X.to(device)\n",
    "            labels=labels.long().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = net(X.view(-1, 1, 28, 28))\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        #test the model:\n",
    "        net.to('cpu')\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in test_loader:\n",
    "                inputs, labels = data\n",
    "                outputs = net(inputs.view(-1, 1, 28, 28))\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted==labels).sum().item()\n",
    "\n",
    "        # return 100*correct/total\n",
    "        session.report({\"mean_accuracy\": 100*correct/total})  # Report to Tune\n",
    "\n",
    "\n",
    "\n",
    "search_space = {\"lr\": tune.uniform(1e-4, 1e-2), \"lastSize\": tune.randint(10,100)}\n",
    "algo = OptunaSearch()\n",
    "\n",
    "trainable_with_resources = tune.with_resources(\n",
    "    tune.with_parameters(trainTestNet,train_set=train_set,test_set=test_set),\n",
    "     {\"cpu\": 1, \"gpu\":1})\n",
    "\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    trainable_with_resources,\n",
    "    tune_config=tune.TuneConfig(\n",
    "        metric=\"mean_accuracy\",\n",
    "        mode=\"max\",\n",
    "        search_alg=algo,\n",
    "        num_samples=1,\n",
    "    ),\n",
    "    run_config=air.RunConfig(\n",
    "        stop={\"training_iteration\": 1},\n",
    "    ),\n",
    "    param_space=search_space,\n",
    ")\n",
    "\n",
    "#if I don't limit num_cpus, ray tries to use the whole node and crashes:\n",
    "ray.init( num_cpus=numCPUs,num_gpus=numGPUs, log_to_driver = False)\n",
    "\n",
    "result_grid = tuner.fit()\n",
    "print(\"Best config is:\", result_grid.get_best_result().config,\n",
    " ' with accuracy: ', result_grid.get_best_result().metrics['mean_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we run this, quite a lot of things get written to the slurm-*.out file, but the bottom part is: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "== Status ==\n",
    "Current time: 2023-09-18 23:15:22 (running for 00:00:13.02)\n",
    "Memory usage on this node: 35.3/503.2 GiB\n",
    "Using FIFO scheduling algorithm.\n",
    "Resources requested: 0/7 CPUs, 0/1 GPUs, 0.0/328.01 GiB heap, 0.0/144.57 GiB objects\n",
    "Current best trial: 110a0a66 with mean_accuracy=19.0 and parameters={'lr': 0.008263221162693079, 'lastSize': 73}\n",
    "Result logdir: /users/mikkelse/ray_results/trainTestNet_2023-09-18_23-15-07\n",
    "Number of trials: 1/1 (1 TERMINATED)\n",
    "+-----------------------+------------+---------------------+------------+------------+-------+--------+------------------+\n",
    "| Trial name            | status     | loc                 |   lastSize |         lr |   acc |   iter |   total time (s) |\n",
    "|-----------------------+------------+---------------------+------------+------------+-------+--------+------------------|\n",
    "| trainTestNet_110a0a66 | TERMINATED | 10.253.41.144:96970 |         73 | 0.00826322 |    19 |      1 |           10.755 |\n",
    "+-----------------------+------------+---------------------+------------+------------+-------+--------+------------------+\n",
    "\n",
    "\n",
    "Best config is: {'lr': 0.008263221162693079, 'lastSize': 73}  with accuracy:  19.0\n",
    "End:\n",
    "------------ Mon 18 Sep 2023 11:15:26 PM EEST ----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to note in the ray.tune-script:\n",
    "- Ray does not understand the concept of SLURM. Particularly the fact that the job may not have the entire node allocated. This means that we have to go in and restrict ray 'manually'. \n",
    "- To be able to share the training data between instances, I wrap 'trainTestNet' in a 'with_parameters' call. This passes these parameters to all instances, but using a shared copy, so we don't have to make a separate version of the data set for each run. In more elaborate setups, for instance where you are reading from disk, this may backfire. If so, I suggest reading up on threadsafe data loaders, for instance here: https://docs.ray.io/en/latest/tune/examples/tune-pytorch-cifar.html\n",
    "\n",
    "### Good habits when working on a cluster\n",
    "Finally, a few pieces of advice for being efficient with your own time on a cluster:\n",
    " - Use a version control system (git) for syncronizing your work computer (where you presumably do most of the coding) with your folder on the cluster (LUMI). This means you will only have to push & pull to make sure that all files are the correct version, and only the minimal of changes need to be transferred. This reduces errors (it's really annoying if you're copying files over manually, but forgot to move one file that was also changed), and makes it much easier for you to keep track of which version of your pile of files was actually used for creating something. \n",
    " - Use a logger. Jobs running on the cluster are not that easy to check on, and output will just be text written to some output file. If you use a logger (such as 'neptune' or 'wandb'), checking in on your code is as easy as opening a browser. An added bonus is that a good logger will also help you keep track of how well your code utilizes the compute resources, what errors and warnings you're getting, and which version of your repository (assuming you followed advice #1) was responsible for creating a certain output. \n",
    " - If you want to inspect a running job, you can use 'srun' to log into the compute node: https://docs.lumi-supercomputer.eu/runjobs/scheduled-jobs/interactive/#using-srun-to-check-running-jobs. In this regard, the command 'rocm_smi' may be useful for you to check what the GPUs are doing. \n",
    "\n",
    "## Getting to use LUMI in your project\n",
    "\n",
    "If you have made it this far in the guide, you should have a decent idea whether you want to use LUMI in your deep learning project. If you do, send me an email before the start of the project period, and I will ask DeiC to set up a project allocation for your group."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  },
  "language_info": {
   "name": "octave"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
