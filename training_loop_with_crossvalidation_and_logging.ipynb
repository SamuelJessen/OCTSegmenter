{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "\n",
    "class OCTDataset(Dataset):\n",
    "    def __init__(self, root_dir, indices=None, train=True, is_gentuity=False, transform=None):\n",
    "        \n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.train = train\n",
    "        self.is_gentuity = is_gentuity\n",
    "        self.transform = transform\n",
    "        \n",
    "        if self.is_gentuity:\n",
    "            # Gentuity dataset has separate train and test folders\n",
    "            split_dir = \"train\" if self.train else \"test\"\n",
    "            self.images_dir = self.root_dir / split_dir / \"images\"\n",
    "            self.masks_dir = self.root_dir / split_dir / \"annotations\"\n",
    "            self.samples = sorted(self.images_dir.glob(\"*.tiff\"))\n",
    "        else:\n",
    "            # Terumo dataset has only train data\n",
    "            self.images_dir = self.root_dir / \"train\" / \"images\"\n",
    "            self.masks_dir = self.root_dir / \"train\" / \"annotations\"\n",
    "            self.samples = sorted(self.images_dir.glob(\"*.tiff\"))\n",
    "        \n",
    "        # Filter image paths using indices if provided\n",
    "        if indices is not None:\n",
    "            self.samples = [self.samples[i] for i in indices]\n",
    "        \n",
    "        else:\n",
    "            self.samples = sorted(self.images_dir.glob(\"*.tiff\"))\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the image\n",
    "        image_path = self.samples[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")  # Keep it as a PIL Image\n",
    "        # Get image height and width\n",
    "        width, height = image.size\n",
    "\n",
    "        # Load the corresponding mask\n",
    "        mask_path = self.masks_dir / f\"{image_path.stem}.json\"\n",
    "        with open(mask_path, \"r\") as f:\n",
    "            mask_data = json.load(f)\n",
    "        \n",
    "        # Create a binary mask (0 and 1 values)\n",
    "        mask = np.zeros((height, width), dtype=np.uint8)  # image.size gives (width, height)\n",
    "        for coord in mask_data[\"mask\"]:\n",
    "            x, y = coord\n",
    "            if 0 <= x < mask.shape[0] and 0 <= y < mask.shape[1]:\n",
    "                mask[x, y] = 1\n",
    "        \n",
    "        mask = np.clip(mask, 0, 1).astype(np.uint8)\n",
    "        \n",
    "        mask = Image.fromarray(mask*255)\n",
    "        \n",
    "        # Apply the transformation if available\n",
    "        if self.transform:\n",
    "            # Convert image and mask to Tensor\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "        \n",
    "        # Add channel dimension for mask\n",
    "        mask = mask.unsqueeze(0) if len(mask.shape) == 2 else mask  # Add channel if needed\n",
    "\n",
    "        unique_id = mask_data[\"unique_id\"]\n",
    "        \n",
    "        return image, mask, image_path.stem, unique_id  # Returning image filename too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels // 8, in_channels, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Compute the attention weights\n",
    "        attention = self.sigmoid(self.conv2(self.sigmoid(self.conv1(x))))\n",
    "        return x * attention  # Element-wise multiplication\n",
    "\n",
    "# Define a large U-Net model with Attn\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_channels=3, output_channels=1, initial_filters=64):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.encoder1 = self.conv_block(input_channels, initial_filters)\n",
    "        self.encoder2 = self.conv_block(initial_filters, initial_filters * 2)\n",
    "        self.encoder3 = self.conv_block(initial_filters * 2, initial_filters * 4)\n",
    "        self.encoder4 = self.conv_block(initial_filters * 4, initial_filters * 8)\n",
    "        self.encoder5 = self.conv_block(initial_filters * 8, initial_filters * 16)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "        # Adjusting channels to match attention input\n",
    "        self.upconv5 = nn.ConvTranspose2d(initial_filters * 16, initial_filters * 8, kernel_size=2, stride=2)\n",
    "        self.attn5 = AttentionBlock(initial_filters * 16)  # Updated input channels\n",
    "        self.decoder5 = self.conv_block(initial_filters * 16, initial_filters * 8)\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(initial_filters * 8, initial_filters * 4, kernel_size=2, stride=2)\n",
    "        self.attn4 = AttentionBlock(initial_filters * 8)  # Updated input channels\n",
    "        self.decoder4 = self.conv_block(initial_filters * 8, initial_filters * 4)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(initial_filters * 4, initial_filters * 2, kernel_size=2, stride=2)\n",
    "        self.attn3 = AttentionBlock(initial_filters * 4)  # Updated input channels\n",
    "        self.decoder3 = self.conv_block(initial_filters * 4, initial_filters * 2)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(initial_filters * 2, initial_filters, kernel_size=2, stride=2)\n",
    "        self.attn2 = AttentionBlock(initial_filters * 2)  # Updated input channels\n",
    "        self.decoder2 = self.conv_block(initial_filters * 2, initial_filters)\n",
    "\n",
    "        self.final_conv = nn.Conv2d(initial_filters, output_channels, kernel_size=1)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoding path\n",
    "        e1 = self.encoder1(x)\n",
    "        e2 = self.encoder2(self.pool(e1))\n",
    "        e3 = self.encoder3(self.pool(e2))\n",
    "        e4 = self.encoder4(self.pool(e3))\n",
    "        e5 = self.encoder5(self.pool(e4))\n",
    "\n",
    "        # Decoding path with skip connections and attention\n",
    "        d5 = torch.cat((self.upconv5(e5), e4), dim=1)\n",
    "        d5 = self.attn5(d5)  # Apply attention\n",
    "        d5 = self.decoder5(d5)\n",
    "\n",
    "        d4 = torch.cat((self.upconv4(d5), e3), dim=1)\n",
    "        d4 = self.attn4(d4)  # Apply attention\n",
    "        d4 = self.decoder4(d4)\n",
    "\n",
    "        d3 = torch.cat((self.upconv3(d4), e2), dim=1)\n",
    "        d3 = self.attn3(d3)  # Apply attention\n",
    "        d3 = self.decoder3(d3)\n",
    "\n",
    "        d2 = torch.cat((self.upconv2(d3), e1), dim=1)\n",
    "        d2 = self.attn2(d2)  # Apply attention\n",
    "        d2 = self.decoder2(d2)\n",
    "\n",
    "        # Final output layer\n",
    "        out = self.final_conv(d2)\n",
    "\n",
    "        return torch.sigmoid(out)  # Assuming binary segmentation\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):   \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        # Calculate intersection and union\n",
    "        intersection = torch.sum(inputs * targets)\n",
    "        union = torch.sum(inputs) + torch.sum(targets)\n",
    "\n",
    "        # Compute Dice Loss\n",
    "        dice_loss = 1 - (2. * intersection + smooth) / (union + smooth)  \n",
    "        \n",
    "        return dice_loss\n",
    "\n",
    "class DiceBCELoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceBCELoss, self).__init__()\n",
    "        self.bce_loss = nn.BCELoss()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1): \n",
    "        #flatten label and prediction tensors\n",
    "        inputs_flatten = inputs.view(-1)\n",
    "        targets_flatten = targets.view(-1)\n",
    "        \n",
    "        # Calculate intersection and union\n",
    "        intersection = torch.sum(inputs_flatten * targets_flatten)\n",
    "        union = torch.sum(inputs_flatten) + torch.sum(targets_flatten)\n",
    "\n",
    "        # Compute BCE Loss\n",
    "        bce = self.bce_loss(inputs, targets)\n",
    "\n",
    "        # Compute Dice Loss\n",
    "        dice_loss = 1 - (2. * intersection + smooth) / (union + smooth)  \n",
    "        Dice_BCE = bce + dice_loss\n",
    "        \n",
    "        return Dice_BCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tempfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from filelock import FileLock\n",
    "from torch.utils.data import random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from typing import Dict\n",
    "import ray\n",
    "from ray import train, tune\n",
    "from ray.train import Checkpoint\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray import tune, air\n",
    "from ray.air import session\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import neptune\n",
    "os.environ[\"TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S\"] = \"0\"\n",
    "\n",
    "def train_model_cv(config):\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    net = Net().to(device)\n",
    "\n",
    "    # Select optimizer based on the configuration\n",
    "    if config[\"optimizer\"] == \"AdamW\":\n",
    "        optimizer = optim.AdamW(net.parameters(), lr=config[\"lr\"])\n",
    "    elif config[\"optimizer\"] == \"SGD\":\n",
    "        optimizer = optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9)\n",
    "    elif config[\"optimizer\"] == \"RMSprop\":\n",
    "        optimizer = optim.RMSprop(net.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    # Select loss function based on the configuration\n",
    "    if config[\"loss_function\"] == \"DiceLoss\":\n",
    "        criterion = DiceLoss()\n",
    "    elif config[\"loss_function\"] == \"DiceBCELoss\":\n",
    "        criterion = DiceBCELoss()\n",
    "    elif config[\"loss_function\"] == \"BCELoss\":\n",
    "        criterion = nn.BCELoss()\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256), interpolation=Image.NEAREST),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    # Load existing checkpoint through `get_checkpoint()` API.\n",
    "    if train.get_checkpoint():\n",
    "        loaded_checkpoint = train.get_checkpoint()\n",
    "        with loaded_checkpoint.as_directory() as loaded_checkpoint_dir:\n",
    "            model_state, optimizer_state = torch.load(\n",
    "                os.path.join(loaded_checkpoint_dir, \"checkpoint.pt\")\n",
    "            )\n",
    "            net.load_state_dict(model_state)\n",
    "            optimizer.load_state_dict(optimizer_state)\n",
    "\n",
    "    root_dir = config[\"root_dir\"]\n",
    "    folds= config[\"folds\"]\n",
    "    \n",
    "    with open(os.path.join(root_dir, \"metadata.csv\"), \"r\") as f:\n",
    "        metadata_df = pd.read_csv(f)\n",
    "        skf = StratifiedKFold(n_splits=folds)\n",
    "        splits = list(skf.split(metadata_df, metadata_df[\"unique_id\"]))\n",
    "\n",
    "    for fold in range(folds):\n",
    "        # Train and validate the model\n",
    "        print(f\"Training on fold {fold+1} out of {folds}\")\n",
    "        \n",
    "        # Initialize Neptune run\n",
    "        run = neptune.init_run(\n",
    "            project=\"OCTAA/OCTSegmenter\",\n",
    "            api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI2MGU2NGNjMi0yNWE0LTRjNzgtOGNlNS1hZDdkMjJhYzYxMWUifQ==\",\n",
    "            name=\"training_and_validation\",\n",
    "            tags=\"terumo\",\n",
    "        )  # your credentials\n",
    "\n",
    "        run[\"sys/group_tags\"].add([\n",
    "            str(config[\"loss_function\"]), \n",
    "            str(config[\"optimizer\"]), \n",
    "            f\"Fold: {str(fold)}\"\n",
    "        ])  # Group tags\n",
    "\n",
    "        # Log configuration parameters\n",
    "        run[\"parameters\"] = config\n",
    "\n",
    "        train_indices, val_indices = splits[fold]\n",
    "\n",
    "        train_dataset = OCTDataset(root_dir, indices=train_indices, transform=transform)\n",
    "        val_dataset = OCTDataset(root_dir, indices=val_indices, transform=transform)\n",
    "\n",
    "        trainloader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "        valloader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "\n",
    "        best_val_loss = float(\"inf\")\n",
    "        epochs = config[\"epochs\"]\n",
    "        no_improvement_epochs = 0\n",
    "        patience = config[\"patience\"]\n",
    "        scheduler = ReduceLROnPlateau(optimizer, factor=0.1, patience=10)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            net.train()\n",
    "            running_loss = 0.0\n",
    "            epoch_steps= 0\n",
    "\n",
    "            for i, data in enumerate(trainloader):\n",
    "                images, masks, _, _ = data\n",
    "                images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = net(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * images.size(0)\n",
    "\n",
    "                epoch_steps += 1\n",
    "                if i % 10 == 9:  # print every 10 mini-batches\n",
    "                    print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1,\n",
    "                                                    loss.item()))\n",
    "\n",
    "\n",
    "            # Calculate training loss and accuracy for the epoch\n",
    "            train_loss = running_loss / len(trainloader.dataset)\n",
    "            run[\"train_loss\"].append(train_loss)  # Log training loss to neptune\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {train_loss:.4f}\")\n",
    "\n",
    "            # Validation phase\n",
    "            net.eval()\n",
    "            val_loss = 0.0\n",
    "            dice_loss = 0.0\n",
    "\n",
    "            with torch.no_grad():  # No need to calculate gradients during validation\n",
    "                for data in valloader:\n",
    "                    images, masks, _, _ = data\n",
    "                    images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "                    outputs = net(images)\n",
    "                    loss = criterion(outputs, masks)\n",
    "                    val_loss += loss.item() * images.size(0)\n",
    "\n",
    "                    # Calculate Dice loss\n",
    "                    dice = DiceLoss()\n",
    "                    loss = dice(outputs, masks)\n",
    "                    dice_loss += loss.item() * images.size(0)\n",
    "            \n",
    "            # Calculate validation loss and accuracy\n",
    "            val_loss = val_loss / len(valloader.dataset)\n",
    "            avg_dice_loss = dice_loss / len(valloader.dataset)\n",
    "            scheduler.step(val_loss) # Adjust learning rate based on validation loss\n",
    "            run[\"val_loss\"].append(val_loss)  # Log validation loss\n",
    "            run[\"dice_loss\"].append(avg_dice_loss)  # Log Dice loss\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "            with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "                path = os.path.join(temp_checkpoint_dir, \"checkpoint.pt\")\n",
    "                torch.save(\n",
    "                    (net.state_dict(), optimizer.state_dict()), path\n",
    "                )\n",
    "                checkpoint = Checkpoint.from_directory(temp_checkpoint_dir)\n",
    "                train.report(\n",
    "                    {\"loss\": val_loss, \"accuracy\": 1 - avg_dice_loss, \"dice_loss\": avg_dice_loss, \"fold\": fold},\n",
    "                    checkpoint=checkpoint,\n",
    "                )\n",
    "\n",
    "            # Check if validation loss improves\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                no_improvement_epochs = 0\n",
    "                print(f\"Validation loss improved to {val_loss:.4f}. Saving checkpoint.\")\n",
    "                \n",
    "            else:\n",
    "                no_improvement_epochs += 1\n",
    "                print(f\"Validation loss did not improve. Best so far: {best_val_loss:.4f}\")\n",
    "            \n",
    "            if no_improvement_epochs >= patience:\n",
    "                print(f\"Stopping early. No improvement in {patience} epochs.\")\n",
    "                run[\"early_stopping\"] = True\n",
    "                break\n",
    "\n",
    "        run.stop()\n",
    "        print(\"Finished Training\")\n",
    "\n",
    "    \n",
    "def test_best_model(best_result):\n",
    "    \n",
    "    # Initialize Neptune run\n",
    "    run = neptune.init_run(\n",
    "        project=\"OCTAA/OCTSegmenter\",\n",
    "        api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI2MGU2NGNjMi0yNWE0LTRjNzgtOGNlNS1hZDdkMjJhYzYxMWUifQ==\",\n",
    "        name=\"best_model_test\",\n",
    "        tags=\"gentuity\"  \n",
    "    )  # your credentials\n",
    "\n",
    "    # Log configuration parameters\n",
    "    run[\"parameters\"] = best_result.config\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    best_trained_model = Net().to(device)\n",
    "\n",
    "    checkpoint_path = os.path.join(best_result.checkpoint.to_directory(), \"checkpoint.pt\")\n",
    "\n",
    "    model_state, optimizer_state = torch.load(checkpoint_path, weights_only=True)\n",
    "    best_trained_model.load_state_dict(model_state)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256), interpolation=Image.NEAREST),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    root_dir = r\"D:\\OneDrive - Aarhus Universitet\\9. Semester\\Deep Learning\\data_gentuity\"\n",
    "\n",
    "    test_dataset = OCTDataset(root_dir, transform=transform, train=False, is_gentuity=True)\n",
    "    testloader = DataLoader(test_dataset, batch_size=best_result.config[\"batch_size\"], shuffle=False)\n",
    "\n",
    "    criterion = DiceLoss()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for data in testloader:\n",
    "            images, masks, _, _ = data\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "            outputs = best_trained_model(images)\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            loss = criterion(predicted, masks)\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "\n",
    "    # Calculate average loss and accuracy\n",
    "    total_loss /= len(testloader.dataset)\n",
    "    accuracy = 1 - loss\n",
    "\n",
    "    run[\"test_loss\"] = total_loss\n",
    "    run.stop()\n",
    "    print(f\"Test Loss: {total_loss:.4f}, Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-11-28 11:41:14</td></tr>\n",
       "<tr><td>Running for: </td><td>00:02:29.70        </td></tr>\n",
       "<tr><td>Memory:      </td><td>12.6/15.9 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 2.0/8 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  batch_size</th><th>loss_function  </th><th style=\"text-align: right;\">         lr</th><th>optimizer  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">  accuracy</th><th style=\"text-align: right;\">  dice_loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_model_cv_f10aa_00000</td><td>RUNNING </td><td>127.0.0.1:11856</td><td style=\"text-align: right;\">           4</td><td>DiceLoss       </td><td style=\"text-align: right;\">1.44375e-06</td><td>AdamW      </td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         132.168</td><td style=\"text-align: right;\">0.834576</td><td style=\"text-align: right;\">  0.165424</td><td style=\"text-align: right;\">   0.834576</td></tr>\n",
       "<tr><td>train_model_cv_f10aa_00001</td><td>PENDING </td><td>               </td><td style=\"text-align: right;\">           4</td><td>BCELoss        </td><td style=\"text-align: right;\">7.35583e-06</td><td>AdamW      </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td></tr>\n",
       "<tr><td>train_model_cv_f10aa_00002</td><td>PENDING </td><td>               </td><td style=\"text-align: right;\">           4</td><td>DiceBCELoss    </td><td style=\"text-align: right;\">0.000338883</td><td>AdamW      </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td></tr>\n",
       "<tr><td>train_model_cv_f10aa_00003</td><td>PENDING </td><td>               </td><td style=\"text-align: right;\">           4</td><td>DiceLoss       </td><td style=\"text-align: right;\">1.13945e-06</td><td>SGD        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td></tr>\n",
       "<tr><td>train_model_cv_f10aa_00004</td><td>PENDING </td><td>               </td><td style=\"text-align: right;\">           4</td><td>BCELoss        </td><td style=\"text-align: right;\">0.000150183</td><td>SGD        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td></tr>\n",
       "<tr><td>train_model_cv_f10aa_00005</td><td>PENDING </td><td>               </td><td style=\"text-align: right;\">           4</td><td>DiceBCELoss    </td><td style=\"text-align: right;\">0.000150427</td><td>SGD        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td></tr>\n",
       "<tr><td>train_model_cv_f10aa_00006</td><td>PENDING </td><td>               </td><td style=\"text-align: right;\">           4</td><td>DiceLoss       </td><td style=\"text-align: right;\">5.66629e-05</td><td>RMSprop    </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td></tr>\n",
       "<tr><td>train_model_cv_f10aa_00007</td><td>PENDING </td><td>               </td><td style=\"text-align: right;\">           4</td><td>BCELoss        </td><td style=\"text-align: right;\">2.26306e-05</td><td>RMSprop    </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td></tr>\n",
       "<tr><td>train_model_cv_f10aa_00008</td><td>PENDING </td><td>               </td><td style=\"text-align: right;\">           4</td><td>DiceBCELoss    </td><td style=\"text-align: right;\">1.32408e-05</td><td>RMSprop    </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td></tr>\n",
       "<tr><td>train_model_cv_f10aa_00009</td><td>PENDING </td><td>               </td><td style=\"text-align: right;\">           4</td><td>DiceLoss       </td><td style=\"text-align: right;\">1.87621e-05</td><td>AdamW      </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td></tr>\n",
       "<tr><td>train_model_cv_f10aa_00010</td><td>PENDING </td><td>               </td><td style=\"text-align: right;\">           4</td><td>BCELoss        </td><td style=\"text-align: right;\">1.318e-06  </td><td>AdamW      </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td></tr>\n",
       "<tr><td>train_model_cv_f10aa_00011</td><td>PENDING </td><td>               </td><td style=\"text-align: right;\">           4</td><td>DiceBCELoss    </td><td style=\"text-align: right;\">9.61617e-05</td><td>AdamW      </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td></tr>\n",
       "<tr><td>train_model_cv_f10aa_00012</td><td>PENDING </td><td>               </td><td style=\"text-align: right;\">           4</td><td>DiceLoss       </td><td style=\"text-align: right;\">1.56313e-05</td><td>SGD        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td></tr>\n",
       "<tr><td>train_model_cv_f10aa_00013</td><td>PENDING </td><td>               </td><td style=\"text-align: right;\">           4</td><td>BCELoss        </td><td style=\"text-align: right;\">0.00313523 </td><td>SGD        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td></tr>\n",
       "<tr><td>train_model_cv_f10aa_00014</td><td>PENDING </td><td>               </td><td style=\"text-align: right;\">           4</td><td>DiceBCELoss    </td><td style=\"text-align: right;\">0.000811976</td><td>SGD        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td></tr>\n",
       "<tr><td>train_model_cv_f10aa_00015</td><td>PENDING </td><td>               </td><td style=\"text-align: right;\">           4</td><td>DiceLoss       </td><td style=\"text-align: right;\">0.00294079 </td><td>RMSprop    </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td></tr>\n",
       "<tr><td>train_model_cv_f10aa_00016</td><td>PENDING </td><td>               </td><td style=\"text-align: right;\">           4</td><td>BCELoss        </td><td style=\"text-align: right;\">6.21387e-06</td><td>RMSprop    </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td></tr>\n",
       "<tr><td>train_model_cv_f10aa_00017</td><td>PENDING </td><td>               </td><td style=\"text-align: right;\">           4</td><td>DiceBCELoss    </td><td style=\"text-align: right;\">0.00120768 </td><td>RMSprop    </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">           </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model_cv pid=11856)\u001b[0m Training on fold 1 out of 5\n",
      "\u001b[36m(train_model_cv pid=11856)\u001b[0m [neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/OCTAA/OCTSegmenter/e/OCT-183\n",
      "\u001b[36m(train_model_cv pid=11856)\u001b[0m [1,    10] loss: 0.909\n",
      "\u001b[36m(train_model_cv pid=11856)\u001b[0m [1,    20] loss: 0.770\n",
      "\u001b[36m(train_model_cv pid=11856)\u001b[0m [1,    30] loss: 0.908\n",
      "\u001b[36m(train_model_cv pid=11856)\u001b[0m Epoch [1/2], Training Loss: 0.8519\n",
      "\u001b[36m(train_model_cv pid=11856)\u001b[0m Epoch [1/2], Validation Loss: 0.8653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model_cv pid=11856)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=C:/Users/johan/ray_results/train_model_cv_2024-11-28_11-38-34/trial_f10aa_00000_lr=1.4e-06_opt=AdamW_bs=4/checkpoint_000000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model_cv pid=11856)\u001b[0m Validation loss improved to 0.8653. Saving checkpoint.\n",
      "\u001b[36m(train_model_cv pid=11856)\u001b[0m [2,    10] loss: 0.876\n",
      "\u001b[36m(train_model_cv pid=11856)\u001b[0m [2,    20] loss: 0.905\n",
      "\u001b[36m(train_model_cv pid=11856)\u001b[0m [2,    30] loss: 0.914\n",
      "\u001b[36m(train_model_cv pid=11856)\u001b[0m Epoch [2/2], Training Loss: 0.8407\n",
      "\u001b[36m(train_model_cv pid=11856)\u001b[0m Epoch [2/2], Validation Loss: 0.8504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model_cv pid=11856)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=C:/Users/johan/ray_results/train_model_cv_2024-11-28_11-38-34/trial_f10aa_00000_lr=1.4e-06_opt=AdamW_bs=4/checkpoint_000001)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model_cv pid=11856)\u001b[0m Validation loss improved to 0.8504. Saving checkpoint.\n",
      "\u001b[36m(train_model_cv pid=11856)\u001b[0m [neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "\u001b[36m(train_model_cv pid=11856)\u001b[0m [neptune] [info   ] Done!\n",
      "\u001b[36m(train_model_cv pid=11856)\u001b[0m [neptune] [info   ] Waiting for the remaining 8 operations to synchronize with Neptune. Do not kill this process.\n",
      "\u001b[36m(train_model_cv pid=11856)\u001b[0m [neptune] [info   ] All 8 operations synced, thanks for waiting!\n",
      "\u001b[36m(train_model_cv pid=11856)\u001b[0m [neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/OCTAA/OCTSegmenter/e/OCT-183/metadata\n",
      "\u001b[36m(train_model_cv pid=11856)\u001b[0m Finished Training\n",
      "\u001b[36m(train_model_cv pid=11856)\u001b[0m Training on fold 2 out of 5\n",
      "\u001b[36m(train_model_cv pid=11856)\u001b[0m [neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/OCTAA/OCTSegmenter/e/OCT-184\n",
      "\u001b[36m(train_model_cv pid=11856)\u001b[0m [1,    10] loss: 0.858\n",
      "\u001b[36m(train_model_cv pid=11856)\u001b[0m [1,    20] loss: 0.886\n",
      "\u001b[36m(train_model_cv pid=11856)\u001b[0m [1,    30] loss: 0.742\n",
      "\u001b[36m(train_model_cv pid=11856)\u001b[0m Epoch [1/2], Training Loss: 0.8295\n",
      "\u001b[36m(train_model_cv pid=11856)\u001b[0m Epoch [1/2], Validation Loss: 0.8346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model_cv pid=11856)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=C:/Users/johan/ray_results/train_model_cv_2024-11-28_11-38-34/trial_f10aa_00000_lr=1.4e-06_opt=AdamW_bs=4/checkpoint_000002)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model_cv pid=11856)\u001b[0m Validation loss improved to 0.8346. Saving checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 11:41:14,088\tWARNING tune.py:219 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2024-11-28 11:41:14,102\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to 'C:/Users/johan/ray_results/train_model_cv_2024-11-28_11-38-34' in 0.0130s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model_cv pid=11856)\u001b[0m [2,    10] loss: 0.854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 11:41:24,296\tINFO tune.py:1041 -- Total run time: 159.97 seconds (149.68 seconds for the tuning loop).\n",
      "2024-11-28 11:41:24,297\tWARNING tune.py:1056 -- Experiment has been interrupted, but the most recent state was saved.\n",
      "Resume experiment with: Tuner.restore(path=\"C:/Users/johan/ray_results/train_model_cv_2024-11-28_11-38-34\", trainable=...)\n",
      "2024-11-28 11:41:24,326\tWARNING experiment_analysis.py:180 -- Failed to fetch metrics for 17 trial(s):\n",
      "- train_model_cv_f10aa_00001: FileNotFoundError('Could not fetch metrics for train_model_cv_f10aa_00001: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-28_11-38-34/trial_f10aa_00001_lr=7.4e-06_opt=AdamW_bs=4')\n",
      "- train_model_cv_f10aa_00002: FileNotFoundError('Could not fetch metrics for train_model_cv_f10aa_00002: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-28_11-38-34/trial_f10aa_00002_lr=3.4e-04_opt=AdamW_bs=4')\n",
      "- train_model_cv_f10aa_00003: FileNotFoundError('Could not fetch metrics for train_model_cv_f10aa_00003: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-28_11-38-34/trial_f10aa_00003_lr=1.1e-06_opt=SGD_bs=4')\n",
      "- train_model_cv_f10aa_00004: FileNotFoundError('Could not fetch metrics for train_model_cv_f10aa_00004: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-28_11-38-34/trial_f10aa_00004_lr=1.5e-04_opt=SGD_bs=4')\n",
      "- train_model_cv_f10aa_00005: FileNotFoundError('Could not fetch metrics for train_model_cv_f10aa_00005: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-28_11-38-34/trial_f10aa_00005_lr=1.5e-04_opt=SGD_bs=4')\n",
      "- train_model_cv_f10aa_00006: FileNotFoundError('Could not fetch metrics for train_model_cv_f10aa_00006: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-28_11-38-34/trial_f10aa_00006_lr=5.7e-05_opt=RMSprop_bs=4')\n",
      "- train_model_cv_f10aa_00007: FileNotFoundError('Could not fetch metrics for train_model_cv_f10aa_00007: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-28_11-38-34/trial_f10aa_00007_lr=2.3e-05_opt=RMSprop_bs=4')\n",
      "- train_model_cv_f10aa_00008: FileNotFoundError('Could not fetch metrics for train_model_cv_f10aa_00008: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-28_11-38-34/trial_f10aa_00008_lr=1.3e-05_opt=RMSprop_bs=4')\n",
      "- train_model_cv_f10aa_00009: FileNotFoundError('Could not fetch metrics for train_model_cv_f10aa_00009: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-28_11-38-34/trial_f10aa_00009_lr=1.9e-05_opt=AdamW_bs=4')\n",
      "- train_model_cv_f10aa_00010: FileNotFoundError('Could not fetch metrics for train_model_cv_f10aa_00010: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-28_11-38-34/trial_f10aa_00010_lr=1.3e-06_opt=AdamW_bs=4')\n",
      "- train_model_cv_f10aa_00011: FileNotFoundError('Could not fetch metrics for train_model_cv_f10aa_00011: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-28_11-38-34/trial_f10aa_00011_lr=9.6e-05_opt=AdamW_bs=4')\n",
      "- train_model_cv_f10aa_00012: FileNotFoundError('Could not fetch metrics for train_model_cv_f10aa_00012: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-28_11-38-34/trial_f10aa_00012_lr=1.6e-05_opt=SGD_bs=4')\n",
      "- train_model_cv_f10aa_00013: FileNotFoundError('Could not fetch metrics for train_model_cv_f10aa_00013: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-28_11-38-34/trial_f10aa_00013_lr=3.1e-03_opt=SGD_bs=4')\n",
      "- train_model_cv_f10aa_00014: FileNotFoundError('Could not fetch metrics for train_model_cv_f10aa_00014: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-28_11-38-34/trial_f10aa_00014_lr=8.1e-04_opt=SGD_bs=4')\n",
      "- train_model_cv_f10aa_00015: FileNotFoundError('Could not fetch metrics for train_model_cv_f10aa_00015: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-28_11-38-34/trial_f10aa_00015_lr=2.9e-03_opt=RMSprop_bs=4')\n",
      "- train_model_cv_f10aa_00016: FileNotFoundError('Could not fetch metrics for train_model_cv_f10aa_00016: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-28_11-38-34/trial_f10aa_00016_lr=6.2e-06_opt=RMSprop_bs=4')\n",
      "- train_model_cv_f10aa_00017: FileNotFoundError('Could not fetch metrics for train_model_cv_f10aa_00017: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-28_11-38-34/trial_f10aa_00017_lr=1.2e-03_opt=RMSprop_bs=4')\n",
      "[neptune] [warning] NeptuneWarning: By default, these monitoring options are disabled in interactive sessions: 'capture_stdout', 'capture_stderr', 'capture_traceback', 'capture_hardware_metrics'. You can set them to 'True' when initializing the run and the monitoring will continue until you call run.stop() or the kernel stops. NOTE: To track the source files, pass their paths to the 'source_code' argument. For help, see: https://docs.neptune.ai/logging/source_code/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial config: {'root_dir': 'D:\\\\OneDrive - Aarhus Universitet\\\\9. Semester\\\\Deep Learning\\\\data_terumo_smoke_test', 'lr': 1.4437474936625422e-06, 'epochs': 2, 'smoke_test': True, 'batch_size': 4, 'optimizer': 'AdamW', 'folds': 5, 'patience': 20, 'loss_function': 'DiceLoss'}\n",
      "Best trial final validation loss: 0.8345764146910774\n",
      "Best trial final validation accuracy: 0.16542358530892265\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/OCTAA/OCTSegmenter/e/OCT-185\n"
     ]
    }
   ],
   "source": [
    "from ray.train import RunConfig, CheckpointConfig\n",
    "\n",
    "def plot_cv_indices(cv, X, y, ax, n_splits, lw=10, cmap_data=\"tab10\"):\n",
    "    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n",
    "    # Generate the training/testing visualizations for each CV split\n",
    "    for ii, (tr, tt) in enumerate(cv.split(X=X, y=y)):\n",
    "        # Fill in indices with the training/test groups\n",
    "        indices = np.array([np.nan] * len(X))\n",
    "        indices[tt] = 1  # Mark the test samples\n",
    "        indices[tr] = 0  # Mark the train samples\n",
    "\n",
    "        # Visualize the results for the current split\n",
    "        # Train samples in light blue, validation samples in redder orange\n",
    "        ax.scatter(\n",
    "            range(len(indices)),\n",
    "            [ii + 0.5] * len(indices),\n",
    "            c=np.where(indices == 0, '#add8e6', '#ff4500'),  # Set light blue and redder orange\n",
    "            marker=\"_\",\n",
    "            lw=lw,\n",
    "        )\n",
    "\n",
    "    # Plot the unique_id at the end (instead of class labels)\n",
    "    ax.scatter(\n",
    "        range(len(X)), [ii + 1.5] * len(X), c=y, marker=\"_\", lw=lw, cmap=cmap_data\n",
    "    )\n",
    "\n",
    "    # Add a legend for train and validation splits\n",
    "    from matplotlib.lines import Line2D\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], color='#add8e6', lw=4, label='Train'),\n",
    "        Line2D([0], [0], color='#ff4500', lw=4, label='Validation'),\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='upper right', fontsize=12)\n",
    "\n",
    "    # Formatting\n",
    "    yticklabels = list(range(n_splits)) + [\"unique_id\"]\n",
    "    ax.set(\n",
    "        yticks=np.arange(n_splits + 1) + 0.5,\n",
    "        yticklabels=yticklabels,\n",
    "        xlabel=\"Sample index\",\n",
    "        ylabel=\"CV iteration\",\n",
    "        ylim=[n_splits + 1.2, -0.2],\n",
    "        xlim=[0, len(X)],\n",
    "    )\n",
    "    ax.set_title(f\"{type(cv).__name__} Cross-Validation\", fontsize=15)\n",
    "    return ax\n",
    "\n",
    "# Visualize splits\n",
    "def visualize_cv_splits(metadata_df, n_splits=9):\n",
    "    # Extract unique IDs and their corresponding target (unique_id)\n",
    "    unique_ids = metadata_df[\"unique_id\"].values\n",
    "\n",
    "    # Initialize StratifiedKFold with n_splits\n",
    "    skf = StratifiedKFold(n_splits=n_splits)\n",
    "\n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Plot the cross-validation splits\n",
    "    plot_cv_indices(\n",
    "        skf, X=metadata_df, y=unique_ids, ax=ax, n_splits=n_splits\n",
    "    )\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Custom function to shorten trial directory names\n",
    "def trial_dirname_creator(trial):\n",
    "    # Shorten the trial name to only include key parameters\n",
    "    return f\"trial_{trial.trial_id}_lr={trial.config['lr']:.1e}_opt={trial.config['optimizer']}_bs={trial.config['batch_size']}\"\n",
    "\n",
    "def main(num_samples, gpus_per_trial, epochs, smoke_test, folds):\n",
    "    if smoke_test:\n",
    "        root_dir = r\"D:\\OneDrive - Aarhus Universitet\\9. Semester\\Deep Learning\\data_terumo_smoke_test\"\n",
    "        with open(os.path.join(root_dir, \"metadata.csv\"), \"r\") as f:\n",
    "            metadata_df = pd.read_csv(f)\n",
    "            skf = StratifiedKFold(n_splits=folds)\n",
    "            visualize_cv_splits(metadata_df, n_splits=folds)\n",
    "\n",
    "    else:\n",
    "        print(\"Using full dataset\")\n",
    "    \n",
    "    config = {\n",
    "        \"root_dir\": root_dir,\n",
    "        \"lr\": tune.loguniform(1e-6, 1e-2),\n",
    "        \"epochs\": epochs,\n",
    "        \"smoke_test\": smoke_test,\n",
    "        \"batch_size\": tune.choice([4]),\n",
    "        \"optimizer\": tune.grid_search([\"AdamW\", \"SGD\", \"RMSprop\"]),\n",
    "        \"folds\": folds,\n",
    "        \"patience\": 20,\n",
    "        \"loss_function\": tune.grid_search([\"DiceLoss\", \"BCELoss\", \"DiceBCELoss\"])\n",
    "    }\n",
    "\n",
    "    # ASHA SCHEDULER, BUT WILL NOT BE USED\n",
    "    # scheduler = ASHAScheduler(\n",
    "    #     max_t=5,\n",
    "    #     grace_period=5,\n",
    "    #     reduction_factor=2\n",
    "    # )\n",
    "\n",
    "    # Define your checkpoint configuration\n",
    "    checkpoint_config = CheckpointConfig(\n",
    "        num_to_keep=1,  # Only keep the best checkpoint\n",
    "        checkpoint_score_attribute=\"loss\",  # The metric used to determine the best checkpoint\n",
    "        checkpoint_score_order=\"min\",  # Keep the checkpoint with the lowest loss\n",
    "    )\n",
    "\n",
    "    # Define the run config with the checkpoint config\n",
    "    run_config = RunConfig(checkpoint_config=checkpoint_config)\n",
    "\n",
    "    tuner = tune.Tuner(\n",
    "        tune.with_resources(\n",
    "            tune.with_parameters(train_model_cv),\n",
    "            resources={\"cpu\": 2, \"gpu\": gpus_per_trial}\n",
    "        ),\n",
    "        tune_config=tune.TuneConfig(\n",
    "            metric=\"loss\",\n",
    "            mode=\"min\",\n",
    "            num_samples=num_samples,\n",
    "            trial_dirname_creator=trial_dirname_creator,\n",
    "        ),\n",
    "        param_space=config,\n",
    "        run_config=run_config,\n",
    "    )\n",
    "    results = tuner.fit()\n",
    "    \n",
    "    best_result = results.get_best_result(\"dice_loss\", \"min\")\n",
    "\n",
    "    print(\"Best trial config: {}\".format(best_result.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(\n",
    "        best_result.metrics[\"loss\"]))\n",
    "    print(\"Best trial final validation accuracy: {}\".format(\n",
    "        best_result.metrics[\"accuracy\"]))\n",
    "\n",
    "    test_best_model(best_result)\n",
    "\n",
    "main(num_samples=2, gpus_per_trial=1, epochs=2, smoke_test=True, folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading results from C:\\Users\\johan\\ray_results\\train_model_cv_2024-11-26_18-30-35...\n",
      "One of the trials failed!\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[WinError 112] Failed copying 'C:/Users/johan/ray_results/train_model_cv_2024-11-26_18-30-35/trial_2b60a_00035_lr=1.0e-04_opt=RMSprop_bs=4/checkpoint_000004/checkpoint.pt' to 'C:/Users/johan/AppData/Local/Temp/checkpoint_tmp_eb1ac5460b9145e2847271a019179487/checkpoint.pt'. Detail: [Windows error 112] Der er ikke tilstrækkelig plads på disken.\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Load the best checkpoint if it exists\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m best_result\u001b[38;5;241m.\u001b[39mcheckpoint:\n\u001b[1;32m---> 24\u001b[0m     checkpoint_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(best_result\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mto_directory(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo checkpoint found for the best result.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\johan\\anaconda3\\envs\\dl\\Lib\\site-packages\\ray\\train\\_checkpoint.py:216\u001b[0m, in \u001b[0;36mCheckpoint.to_directory\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# Timeout 0 means there will be only one attempt to acquire\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# the file lock. If it cannot be acquired, throw a TimeoutError\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m TempFileLock(local_path, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m--> 216\u001b[0m         _download_from_fs_path(\n\u001b[0;32m    217\u001b[0m             fs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilesystem, fs_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath, local_path\u001b[38;5;241m=\u001b[39mlocal_path\n\u001b[0;32m    218\u001b[0m         )\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m:\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# if the directory is already locked, then wait but do not do anything.\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m TempFileLock(local_path, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\johan\\anaconda3\\envs\\dl\\Lib\\site-packages\\ray\\train\\_internal\\storage.py:184\u001b[0m, in \u001b[0;36m_download_from_fs_path\u001b[1;34m(fs, fs_path, local_path, filelock)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exists_before:\n\u001b[0;32m    183\u001b[0m     shutil\u001b[38;5;241m.\u001b[39mrmtree(local_path, ignore_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 184\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32mc:\\Users\\johan\\anaconda3\\envs\\dl\\Lib\\site-packages\\ray\\train\\_internal\\storage.py:177\u001b[0m, in \u001b[0;36m_download_from_fs_path\u001b[1;34m(fs, fs_path, local_path, filelock)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filelock:\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m TempFileLock(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mnormpath(local_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.lock\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 177\u001b[0m         _pyarrow_fs_copy_files(fs_path, local_path, source_filesystem\u001b[38;5;241m=\u001b[39mfs)\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    179\u001b[0m     _pyarrow_fs_copy_files(fs_path, local_path, source_filesystem\u001b[38;5;241m=\u001b[39mfs)\n",
      "File \u001b[1;32mc:\\Users\\johan\\anaconda3\\envs\\dl\\Lib\\site-packages\\ray\\train\\_internal\\storage.py:110\u001b[0m, in \u001b[0;36m_pyarrow_fs_copy_files\u001b[1;34m(source, destination, source_filesystem, destination_filesystem, **kwargs)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# Use a large chunk size to speed up large checkpoint transfers.\u001b[39;00m\n\u001b[0;32m    108\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunk_size\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m64\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024\u001b[39m)\n\u001b[1;32m--> 110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pyarrow\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39mcopy_files(\n\u001b[0;32m    111\u001b[0m     source,\n\u001b[0;32m    112\u001b[0m     destination,\n\u001b[0;32m    113\u001b[0m     source_filesystem\u001b[38;5;241m=\u001b[39msource_filesystem,\n\u001b[0;32m    114\u001b[0m     destination_filesystem\u001b[38;5;241m=\u001b[39mdestination_filesystem,\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    116\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\johan\\anaconda3\\envs\\dl\\Lib\\site-packages\\pyarrow\\fs.py:259\u001b[0m, in \u001b[0;36mcopy_files\u001b[1;34m(source, destination, source_filesystem, destination_filesystem, chunk_size, use_threads)\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_info\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m FileType\u001b[38;5;241m.\u001b[39mDirectory:\n\u001b[0;32m    258\u001b[0m     source_sel \u001b[38;5;241m=\u001b[39m FileSelector(source_path, recursive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 259\u001b[0m     _copy_files_selector(source_fs, source_sel,\n\u001b[0;32m    260\u001b[0m                          destination_fs, destination_path,\n\u001b[0;32m    261\u001b[0m                          chunk_size, use_threads)\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    263\u001b[0m     _copy_files(source_fs, source_path,\n\u001b[0;32m    264\u001b[0m                 destination_fs, destination_path,\n\u001b[0;32m    265\u001b[0m                 chunk_size, use_threads)\n",
      "File \u001b[1;32mc:\\Users\\johan\\anaconda3\\envs\\dl\\Lib\\site-packages\\pyarrow\\_fs.pyx:1630\u001b[0m, in \u001b[0;36mpyarrow._fs._copy_files_selector\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\johan\\anaconda3\\envs\\dl\\Lib\\site-packages\\pyarrow\\error.pxi:91\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 112] Failed copying 'C:/Users/johan/ray_results/train_model_cv_2024-11-26_18-30-35/trial_2b60a_00035_lr=1.0e-04_opt=RMSprop_bs=4/checkpoint_000004/checkpoint.pt' to 'C:/Users/johan/AppData/Local/Temp/checkpoint_tmp_eb1ac5460b9145e2847271a019179487/checkpoint.pt'. Detail: [Windows error 112] Der er ikke tilstrækkelig plads på disken.\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from ray import tune\n",
    "from ray.train import Result\n",
    "\n",
    "storage_path = r\"C:\\Users\\johan\\ray_results\"\n",
    "exp_name = \"train_model_cv_2024-11-26_18-30-35\"\n",
    "experiment_path = os.path.join(storage_path, exp_name)\n",
    "print(f\"Loading results from {experiment_path}...\")\n",
    "\n",
    "restored_tuner = tune.Tuner.restore(experiment_path, trainable=train_model_cv)\n",
    "result_grid = restored_tuner.get_results()\n",
    "\n",
    "# Check if there have been errors\n",
    "if result_grid.errors:\n",
    "    print(\"One of the trials failed!\")\n",
    "else:\n",
    "    print(\"No errors!\")\n",
    "    \n",
    "# Get the result with the maximum test set `mean_accuracy`\n",
    "best_result: Result = result_grid.get_best_result()\n",
    "\n",
    "# Load the best checkpoint if it exists\n",
    "if best_result.checkpoint:\n",
    "    checkpoint_path = os.path.join(best_result.checkpoint.to_directory(), \"checkpoint.pt\")\n",
    "else:\n",
    "    raise ValueError(\"No checkpoint found for the best result.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-26 16:39:38,781\tWARNING experiment_analysis.py:180 -- Failed to fetch metrics for 40 trial(s):\n",
      "- train_model_cv_83259_00005: FileNotFoundError('Could not fetch metrics for train_model_cv_83259_00005: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-26_16-31-22/trial_83259_00005_lr=1.0e-04_opt=Adam_bs=4')\n",
      "- train_model_cv_83259_00022: FileNotFoundError('Could not fetch metrics for train_model_cv_83259_00022: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-26_16-31-22/trial_83259_00022_lr=1.0e-04_opt=SGD_bs=4')\n",
      "- train_model_cv_83259_00017: FileNotFoundError('Could not fetch metrics for train_model_cv_83259_00017: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-26_16-31-22/trial_83259_00017_lr=1.0e-04_opt=SGD_bs=4')\n",
      "- train_model_cv_83259_00033: FileNotFoundError('Could not fetch metrics for train_model_cv_83259_00033: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-26_16-31-22/trial_83259_00033_lr=1.0e-04_opt=RMSprop_bs=4')\n",
      "- train_model_cv_83259_00015: FileNotFoundError('Could not fetch metrics for train_model_cv_83259_00015: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-26_16-31-22/trial_83259_00015_lr=1.0e-04_opt=SGD_bs=4')\n",
      "- train_model_cv_83259_00010: FileNotFoundError('Could not fetch metrics for train_model_cv_83259_00010: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-26_16-31-22/trial_83259_00010_lr=1.0e-04_opt=Adam_bs=4')\n",
      "- train_model_cv_83259_00009: FileNotFoundError('Could not fetch metrics for train_model_cv_83259_00009: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-26_16-31-22/trial_83259_00009_lr=1.0e-04_opt=Adam_bs=4')\n",
      "- train_model_cv_83259_00013: FileNotFoundError('Could not fetch metrics for train_model_cv_83259_00013: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-26_16-31-22/trial_83259_00013_lr=1.0e-04_opt=Adam_bs=4')\n",
      "- train_model_cv_83259_00023: FileNotFoundError('Could not fetch metrics for train_model_cv_83259_00023: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-26_16-31-22/trial_83259_00023_lr=1.0e-04_opt=SGD_bs=4')\n",
      "- train_model_cv_83259_00029: FileNotFoundError('Could not fetch metrics for train_model_cv_83259_00029: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-26_16-31-22/trial_83259_00029_lr=1.0e-04_opt=SGD_bs=4')\n",
      "- train_model_cv_83259_00030: FileNotFoundError('Could not fetch metrics for train_model_cv_83259_00030: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-26_16-31-22/trial_83259_00030_lr=1.0e-04_opt=RMSprop_bs=4')\n",
      "- train_model_cv_83259_00040: FileNotFoundError('Could not fetch metrics for train_model_cv_83259_00040: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-26_16-31-22/trial_83259_00040_lr=1.0e-04_opt=RMSprop_bs=4')\n",
      "- train_model_cv_83259_00044: FileNotFoundError('Could not fetch metrics for train_model_cv_83259_00044: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-26_16-31-22/trial_83259_00044_lr=1.0e-04_opt=RMSprop_bs=4')\n",
      "- train_model_cv_83259_00034: FileNotFoundError('Could not fetch metrics for train_model_cv_83259_00034: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-26_16-31-22/trial_83259_00034_lr=1.0e-04_opt=RMSprop_bs=4')\n",
      "- train_model_cv_83259_00020: FileNotFoundError('Could not fetch metrics for train_model_cv_83259_00020: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-26_16-31-22/trial_83259_00020_lr=1.0e-04_opt=SGD_bs=4')\n",
      "- train_model_cv_83259_00025: FileNotFoundError('Could not fetch metrics for train_model_cv_83259_00025: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-26_16-31-22/trial_83259_00025_lr=1.0e-04_opt=SGD_bs=4')\n",
      "- train_model_cv_83259_00028: FileNotFoundError('Could not fetch metrics for train_model_cv_83259_00028: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-26_16-31-22/trial_83259_00028_lr=1.0e-04_opt=SGD_bs=4')\n",
      "- train_model_cv_83259_00032: FileNotFoundError('Could not fetch metrics for train_model_cv_83259_00032: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-26_16-31-22/trial_83259_00032_lr=1.0e-04_opt=RMSprop_bs=4')\n",
      "- train_model_cv_83259_00039: FileNotFoundError('Could not fetch metrics for train_model_cv_83259_00039: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-26_16-31-22/trial_83259_00039_lr=1.0e-04_opt=RMSprop_bs=4')\n",
      "- train_model_cv_83259_00031: FileNotFoundError('Could not fetch metrics for train_model_cv_83259_00031: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-26_16-31-22/trial_83259_00031_lr=1.0e-04_opt=RMSprop_bs=4')\n",
      "- train_model_cv_83259_00035: FileNotFoundError('Could not fetch metrics for train_model_cv_83259_00035: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-26_16-31-22/trial_83259_00035_lr=1.0e-04_opt=RMSprop_bs=4')\n",
      "- train_model_cv_83259_00036: FileNotFoundError('Could not fetch metrics for train_model_cv_83259_00036: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-26_16-31-22/trial_83259_00036_lr=1.0e-04_opt=RMSprop_bs=4')\n",
      "- train_model_cv_83259_00012: FileNotFoundError('Could not fetch metrics for train_model_cv_83259_00012: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-26_16-31-22/trial_83259_00012_lr=1.0e-04_opt=Adam_bs=4')\n",
      "- train_model_cv_83259_00016: FileNotFoundError('Could not fetch metrics for train_model_cv_83259_00016: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-26_16-31-22/trial_83259_00016_lr=1.0e-04_opt=SGD_bs=4')\n",
      "- train_model_cv_83259_00019: FileNotFoundError('Could not fetch metrics for train_model_cv_83259_00019: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-26_16-31-22/trial_83259_00019_lr=1.0e-04_opt=SGD_bs=4')\n",
      "- train_model_cv_83259_00024: FileNotFoundError('Could not fetch metrics for train_model_cv_83259_00024: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-26_16-31-22/trial_83259_00024_lr=1.0e-04_opt=SGD_bs=4')\n",
      "- train_model_cv_83259_00042: FileNotFoundError('Could not fetch metrics for train_model_cv_83259_00042: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-26_16-31-22/trial_83259_00042_lr=1.0e-04_opt=RMSprop_bs=4')\n",
      "- train_model_cv_83259_00018: FileNotFoundError('Could not fetch metrics for train_model_cv_83259_00018: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-26_16-31-22/trial_83259_00018_lr=1.0e-04_opt=SGD_bs=4')\n",
      "- train_model_cv_83259_00011: FileNotFoundError('Could not fetch metrics for train_model_cv_83259_00011: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-26_16-31-22/trial_83259_00011_lr=1.0e-04_opt=Adam_bs=4')\n",
      "- train_model_cv_83259_00021: FileNotFoundError('Could not fetch metrics for train_model_cv_83259_00021: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-26_16-31-22/trial_83259_00021_lr=1.0e-04_opt=SGD_bs=4')\n",
      "- train_model_cv_83259_00038: FileNotFoundError('Could not fetch metrics for train_model_cv_83259_00038: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-26_16-31-22/trial_83259_00038_lr=1.0e-04_opt=RMSprop_bs=4')\n",
      "- train_model_cv_83259_00006: FileNotFoundError('Could not fetch metrics for train_model_cv_83259_00006: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-26_16-31-22/trial_83259_00006_lr=1.0e-04_opt=Adam_bs=4')\n",
      "- train_model_cv_83259_00014: FileNotFoundError('Could not fetch metrics for train_model_cv_83259_00014: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-26_16-31-22/trial_83259_00014_lr=1.0e-04_opt=Adam_bs=4')\n",
      "- train_model_cv_83259_00027: FileNotFoundError('Could not fetch metrics for train_model_cv_83259_00027: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-26_16-31-22/trial_83259_00027_lr=1.0e-04_opt=SGD_bs=4')\n",
      "- train_model_cv_83259_00041: FileNotFoundError('Could not fetch metrics for train_model_cv_83259_00041: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-26_16-31-22/trial_83259_00041_lr=1.0e-04_opt=RMSprop_bs=4')\n",
      "- train_model_cv_83259_00043: FileNotFoundError('Could not fetch metrics for train_model_cv_83259_00043: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-26_16-31-22/trial_83259_00043_lr=1.0e-04_opt=RMSprop_bs=4')\n",
      "- train_model_cv_83259_00007: FileNotFoundError('Could not fetch metrics for train_model_cv_83259_00007: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-26_16-31-22/trial_83259_00007_lr=1.0e-04_opt=Adam_bs=4')\n",
      "- train_model_cv_83259_00026: FileNotFoundError('Could not fetch metrics for train_model_cv_83259_00026: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-26_16-31-22/trial_83259_00026_lr=1.0e-04_opt=SGD_bs=4')\n",
      "- train_model_cv_83259_00037: FileNotFoundError('Could not fetch metrics for train_model_cv_83259_00037: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-26_16-31-22/trial_83259_00037_lr=1.0e-04_opt=RMSprop_bs=4')\n",
      "- train_model_cv_83259_00008: FileNotFoundError('Could not fetch metrics for train_model_cv_83259_00008: both result.json and progress.csv were not found at C:/Users/johan/ray_results/train_model_cv_2024-11-26_16-31-22/trial_83259_00008_lr=1.0e-04_opt=Adam_bs=4')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading results from C:\\Users\\johan\\ray_results\\train_model_cv_2024-11-26_16-31-22...\n",
      "One of the trials failed!\n",
      "Number of results: 45\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrial #\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m had an error:\u001b[39m\u001b[38;5;124m\"\u001b[39m, result\u001b[38;5;241m.\u001b[39merror)\n\u001b[0;32m     25\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrial #\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m finished successfully with a loss of: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;241m.\u001b[39mmetrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     29\u001b[0m results_df \u001b[38;5;241m=\u001b[39m result_grid\u001b[38;5;241m.\u001b[39mget_dataframe()\n\u001b[0;32m     30\u001b[0m results_df[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_iteration\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'loss'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from ray import tune\n",
    "from ray.train import Result\n",
    "\n",
    "storage_path = r\"C:\\Users\\johan\\ray_results\"\n",
    "exp_name = \"train_model_cv_2024-11-26_16-31-22\"\n",
    "experiment_path = os.path.join(storage_path, exp_name)\n",
    "print(f\"Loading results from {experiment_path}...\")\n",
    "\n",
    "restored_tuner = tune.Tuner.restore(experiment_path, trainable=train_model_cv)\n",
    "result_grid = restored_tuner.get_results()\n",
    "\n",
    "# Check if there have been errors\n",
    "if result_grid.errors:\n",
    "    print(\"One of the trials failed!\")\n",
    "else:\n",
    "    print(\"No errors!\")\n",
    "\n",
    "num_results = len(result_grid)\n",
    "print(\"Number of results:\", num_results)\n",
    "\n",
    "# Iterate over results\n",
    "for i, result in enumerate(result_grid):\n",
    "    if result.error:\n",
    "        print(f\"Trial #{i} had an error:\", result.error)\n",
    "        continue\n",
    "\n",
    "    print(f\"Trial #{i} finished successfully with a loss of: {result.metrics['loss']}\")\n",
    "\n",
    "results_df = result_grid.get_dataframe()\n",
    "results_df[[\"training_iteration\", \"loss\"]]\n",
    "\n",
    "print(\"Shortest training time:\", results_df[\"time_total_s\"].min())\n",
    "print(\"Longest training time:\", results_df[\"time_total_s\"].max())\n",
    "\n",
    "ax = None\n",
    "for result in result_grid:\n",
    "    label = f\"lr={result.config['lr']:.4f}, batch_size={result.config['batch_size']}, optimizer={result.config['optimizer']}, fold={result.config['fold']}\"\n",
    "    if ax is None:\n",
    "        ax = result.metrics_dataframe.plot(\"training_iteration\", \"loss\", label=label)\n",
    "    else:\n",
    "        result.metrics_dataframe.plot(\"training_iteration\", \"loss\", ax=ax, label=label)\n",
    "ax.set_title(\"Loss vs. Training Iteration for All Trials\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "\n",
    "# Get the result with the maximum test set `mean_accuracy`\n",
    "best_result: Result = result_grid.get_best_result()\n",
    "\n",
    "print(\"Best trial config: {}\".format(best_result.config))\n",
    "\n",
    "# Get the best trial's final loss and accuracy\n",
    "best_loss = best_result.metrics[\"loss\"]\n",
    "print(\"Best trial final test set loss: {}\".format(best_loss))\n",
    "best_accuracy = best_result.metrics[\"accuracy\"]\n",
    "print(\"Best trial final test set accuracy: {}\".format(best_accuracy))\n",
    "\n",
    "# Load the best model\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "best_trained_model = Net().to(device)\n",
    "\n",
    "# Load the best checkpoint\n",
    "checkpoint_path = os.path.join(best_result.checkpoint.to_directory(), \"checkpoint.pt\")\n",
    "\n",
    "model_state, optimizer_state = torch.load(checkpoint_path, weights_only=True)\n",
    "best_trained_model.load_state_dict(model_state)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "best_trained_model.eval()\n",
    "\n",
    "# Load a sample image from the test dataset\n",
    "root_dir = r\"D:\\OneDrive - Aarhus Universitet\\9. Semester\\Deep Learning\\data_gentuity\"\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256), interpolation=Image.NEAREST),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "test_dataset = OCTDataset(root_dir, transform=transform, train=False, is_gentuity=True)\n",
    "random_indices = np.random.choice(len(test_dataset), 1, replace=False)\n",
    "sample_image, sample_mask, _, _ = test_dataset[random_indices[0]]  # Change the index to load a different sample\n",
    "\n",
    "# Move the sample image to the appropriate device\n",
    "sample_image = sample_image.to(device).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Make a prediction\n",
    "with torch.no_grad():\n",
    "    prediction = best_trained_model(sample_image)\n",
    "\n",
    "# Convert the prediction to a binary mask\n",
    "predicted_mask = (prediction > 0.5).float()\n",
    "\n",
    "# Plot the sample image, ground truth mask, and predicted mask\n",
    "fig, ax = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "ax[0].imshow(sample_image.squeeze().permute(1, 2, 0).cpu().numpy())\n",
    "ax[0].set_title(\"Sample Image\")\n",
    "ax[0].axis('off')\n",
    "\n",
    "ax[1].imshow(sample_mask.squeeze().cpu().numpy(), cmap='gray')\n",
    "ax[1].set_title(\"Ground Truth Mask\")\n",
    "ax[1].axis('off')\n",
    "\n",
    "ax[2].imshow(predicted_mask.squeeze().cpu().numpy(), cmap='gray')\n",
    "ax[2].set_title(\"Predicted Mask\")\n",
    "ax[2].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
